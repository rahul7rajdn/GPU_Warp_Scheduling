/*
Copyright (c) <2012>, <Georgia Institute of Technology> All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted 
provided that the following conditions are met:

Redistributions of source code must retain the above copyright notice, this list of conditions 
and the following disclaimer.

Redistributions in binary form must reproduce the above copyright notice, this list of 
conditions and the following disclaimer in the documentation and/or other materials provided 
with the distribution.

Neither the name of the <Georgia Institue of Technology> nor the names of its contributors 
may be used to endorse or promote products derived from this software without specific prior 
written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR 
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY 
AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR 
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR 
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR 
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY 
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR 
OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE 
POSSIBILITY OF SUCH DAMAGE.
*/


#include <cassert>
#include <fstream>
#include <zlib.h>
#include <cstring>
#include <iostream>
#include <sstream>
#include <cstdlib>
#include <random>

#include "trace.h"
#include "macsim.h"
#include "core.h"
#include "cache.h"

#include "ram.h"
#include "ccws_vta.h"

using namespace std;

#define ASSERTM(cond, args...)                                    \
do {                                                              \
  if (!(cond)) {                                                  \
    fprintf(stderr, "%s:%d: ASSERT FAILED ", __FILE__, __LINE__); \
    fprintf(stderr, "%s\n", #cond);                               \
    fprintf(stderr, "%s:%d: ASSERT FAILED ", __FILE__, __LINE__); \
    fprintf(stderr, ## args);                                     \
    fprintf(stderr, "\n");                                        \
    exit(15);                                                     \
  }                                                               \
} while (0)

#define t_read_ver "1.3"

macsim::macsim(GPU_Parameter_Set* gpu_params){
  // Initialize Macsim
  m_cycle = 0;
  n_timeout_req = 0;
  n_requests = 0;
  n_responses = 0;
  total_latency = 0;
  n_cache_req = 0;
  n_l1_hits=0;
  n_blocks_total = 0;
  n_correct_ppa_prediction = 0;
  n_total_ppa_prediction = 0;

  m_gpu_params = gpu_params;

  max_block_per_core = m_gpu_params->Max_Block_Per_Core;
  n_of_cores = m_gpu_params->Num_Of_Cores;
  block_scheduling_policy = m_gpu_params->Block_Scheduling_Policy;
  warp_scheduling_policy = m_gpu_params->Warp_Scheduling_Policy;
  m_cycle_per_period = m_gpu_params->Cycle_Per_Period;
  kernel_config_path = m_gpu_params->GPU_Trace_Path;
  n_repeat_kernel = m_gpu_params->N_Repeat;
  l2cache_size  = m_gpu_params->L2Cache_Size;
  l2cache_assoc = m_gpu_params->L2Cache_Assoc;
  l2cache_line_size = m_gpu_params->L2Cache_Line_Size;
  l2cache_banks = m_gpu_params->L2Cache_Banks;

  for (int id=0; id<n_of_cores; id++){
    c_cycle_total[id] = 0;
    c_insts_total[id] = 0;
    c_stall_cycles[id] = 0;
  }

  cout << "Block Scheduling Policy: " << Block_Scheduling_Policy_Types_str[(int)block_scheduling_policy] << endl;
  cout << "Warp Scheduling Policy: " << Warp_Scheduling_Policy_Types_str[(int)warp_scheduling_policy] << endl;

  // Setup trace
  trace_reader_setup();
}

macsim::~macsim(){}


void macsim::trace_reader_setup()
{
  int truncate_size = 0; 
  // Open kernel_config file
  // -------------------------------------------
  // kernel_config.txt Format
  // -------------------------------------------
  // Trace type (nvbit)
  // Trace version (1.4)
  // Multiple Kernels (-1)
  // 1st Kernel's trace config file (trace.txt)
  // 2nd Kernel's trace config file (trace.txt)
  // ....
  // nth Kernel's trace config file (trace.txt)

  ifstream kernel_config_file;
  kernel_config_file.open(kernel_config_path.c_str(), ifstream::in);
  if (kernel_config_file.fail()) {
    ASSERTM(0, "filename:%s cannot be opened\n", kernel_config_path.c_str());
  }

  string trace_type;
  if (!(kernel_config_file >> trace_type))
    ASSERTM(0, "error reading from file:%s", kernel_config_path.c_str());

  int trace_ver = -1;
  if (!(kernel_config_file >> trace_ver) || trace_ver != 14) {
    ASSERTM(0,
            "this version of the simulator supports only version 1.4 of the "
            "GPU traces\n");
  }

  int kernel_count;
  if (!(kernel_config_file >> kernel_count)) {
    ASSERTM(0, "error reading from file:%s", kernel_config_path.c_str());
  }

  printf("trace type : %s\n",trace_type.c_str());


  if (kernel_count == -1) {
    bool startAdding = false;
    string kernel_directory;
    string line;
    while (kernel_config_file >> kernel_directory)
    {
      string kernel_path = kernel_config_path.substr(0, kernel_config_path.find_last_of('/'));
      kernel_path += kernel_directory.substr(
          kernel_directory.rfind('/', kernel_directory.find_last_of('/') - 1),
          kernel_directory.length());
      for (int i = 0; i < n_repeat_kernel; i++)
        kernels_v.push_back(kernel_path);
    }
  }

  for (auto kernel_path: kernels_v) {
    // open TRACE_CONFIG file for every kernel
    // -------------------------------------------
    // (kernel_name)/trace.txt Format
    // --------------------------------------------
    // Trace type (nvbit)
    // Trace version (1.4)
    // # of maximum blocks per core
    // # of Threads
    // 1st Thread ID    | starting instruction count
    // 2nd Thread ID    | starting instruction count
    // ....
    // nth Thread ID    | starting instruction count
    // thread_id = block_id * 65536 (=1<<16) + warp_id
    kernel_info_s kernel_info;

    ifstream trace_config_file;
    trace_config_file.open(kernel_path.c_str(), ifstream::in);
    if (trace_config_file.fail()) {
      ASSERTM(0, "trace_config_file:%s\n", kernel_path.c_str());
    }

    cout << "> trace_path: " << kernel_path << " truncate_size: " << truncate_size << endl;

    string trace_type;
    if (!(trace_config_file >> trace_type))
      ASSERTM(0, "error reading from file:%s", kernel_path.c_str());

    cout << " trace type : " << trace_type<<endl;
    int trace_ver = -1;
    if (!(trace_config_file >> trace_ver) || trace_ver != 14) {
      ASSERTM(0,
              "this version of the simulator supports only version 1.4 of the "
              "GPU traces\n");
    }

    if(!(trace_config_file >> max_block_per_core))
      ASSERTM(0, "error reading from file:%s", kernel_path.c_str());

    int warp_count;
    if (!(trace_config_file >> warp_count))
      ASSERTM(0, "error reading from file:%s", kernel_path.c_str());
    kernel_info.n_of_warp = warp_count;
    if (warp_count <= 0) 
      ASSERTM(0, "invalid thread count:%d", warp_count);

    // read each warp's information (warp id, # of starting instruction)
    for (int ii = 0; ii < warp_count; ++ii) {
      tuple<int, int, int> warp_info;
      int val1, val2;
      if (!(trace_config_file >> val1 >> val2)) {
        ASSERTM(0, "error reading from file:%s ii:%d\n", kernel_path.c_str(), ii);
      }
      warp_info = make_tuple(val1, val2, 0); // warp_id and warp_start_inst_count
      kernel_info.warp_id_v.push_back(warp_info);
    }

    // open trace_info.txt for every kernel
    // -------------------------------------------
    // (kernel_name)/trace_info.txt Format
    // --------------------------------------------
    // 1st Thread ID    | # of Instructions
    // 2nd Thread ID    | # of Instructions
    // ....
    // nth Thread ID    | # of Instructions
    // thread_id = block_id * 65536 (=1<<16) + warp_id

    string kernel_info_path = kernel_path.substr(0, kernel_path.find_last_of('.'));
    kernel_info_path += "_info.txt";

    ifstream trace_info_file;
    trace_info_file.open(kernel_info_path.c_str(), ifstream::in);
    if (trace_info_file.fail()) {
      ASSERTM(0, "trace_info_file:%s\n", kernel_info_path.c_str());
    }

    // read each warp's information (warp id, # of starting instruction)
    for (int ii = 0; ii < warp_count; ++ii) {
      int warp_id; // dummy
      int inst_count;
      if (!(trace_info_file >> warp_id >> inst_count)) {
        cout << warp_id << inst_count << endl;
        ASSERTM(0, "error reading from file:%s ii:%d\n", kernel_path.c_str(), ii);
      }
      get<2>(kernel_info.warp_id_v[ii]) = inst_count;
      kernel_info.inst_count_total += inst_count;
    }

    // Calculate the number of warps per block
    // thread_id = block_id * 65536 (=1<<16) + warp_id
    for (int ii = 0; ii < warp_count; ++ii) {
      if (get<0>(kernel_info.warp_id_v[ii]) < (1 << 16))
        kernel_info.n_warp_per_block++;
      else
        break;
    }
    kernel_info.n_of_block = warp_count / kernel_info.n_warp_per_block;

    kernel_info_v.push_back(kernel_info);
    trace_config_file.close();
    trace_info_file.close();
    cout << "# of blocks: " << kernel_info.n_of_block << ", # of warps: " << warp_count << ", # of Instrs: " << kernel_info.inst_count_total << endl;
    n_blocks_total += kernel_info.n_of_block;
    n_blocks_per_kernel.push_back(kernel_info.n_of_block);
  } // end of kernels_v loop
  kernel_config_file.close();
}

void macsim::inst_event(trace_info_nvbit_small_s* trace_info, int core_id, 
                        int block_id, int warp_id, sim_time_type c_cycle, bool on_response_insert_in_l1, bool on_response_mark_dirty) {
  // Increment counters in core
  if (is_ld(trace_info->m_opcode))
    core_pointers_v[core_id]->ld_req_cnt++;
  if (is_st(trace_info->m_opcode))
    core_pointers_v[core_id]->st_req_cnt++;

  // add request to scoreboard
  GPU_scoreboard_entry sb_entry;
  sb_entry.addr = trace_info->m_mem_addr;
  sb_entry.PC = trace_info->m_inst_addr;
  sb_entry.req_time = m_cycle;
  sb_entry.is_mem = true;
  sb_entry.core_id = core_id;
  sb_entry.warp_id = warp_id;
  sb_entry.mem_queue_id = n_requests;
  sb_entry.insert_in_l1 = on_response_insert_in_l1;
  sb_entry.mark_dirty = on_response_mark_dirty;
  GPU_scoreboard.push_back(sb_entry);

  // Generate memory request
  RAM_request ram_req = {
    .addr = trace_info->m_mem_addr,
    .is_store = !trace_info->m_is_load,
    .access_sz = trace_info->m_mem_access_size,
    .req_time = m_cycle,
    .core_id = core_id,
    .warp_id = warp_id,
    .request_id = n_requests
  };
  gpu_mem_request_queue->push(ram_req);
  
  n_requests++;
}

void macsim::get_mem_response() {
  // check mem response and update entries
  while (gpu_mem_response_queue->size() != 0) {

    // Pop one response from the response queue
    auto response = gpu_mem_response_queue->front();
    gpu_mem_response_queue->pop();

    // track average request response time
    n_responses++;
    uint64_t mem_response_id = response.request_id;
    sim_time_type req_time=0, resp_time=0;

    // Find GPU scoreboard entry corresponding to the response
    for (auto entry = GPU_scoreboard.begin(); entry != GPU_scoreboard.end(); entry++) {
      if (entry->mem_queue_id == mem_response_id) {
        req_time = entry->req_time; //entry.req_time + delay;
        resp_time = m_cycle - req_time;

        // Delegated insert in L2 cache
        Addr line_addr, victim_line_addr;
        cache_data_t* l2_ins_ln = (cache_data_t*) l2cache->insert_cache(entry->addr, &line_addr, &victim_line_addr, 0, false);

        // writeback replaced line if it was valid and dirty
        if(victim_line_addr && l2_ins_ln->m_dirty) {
          // Generate memory request for writeback
          RAM_request ram_req = {
            .addr = victim_line_addr,
            .is_store = true,
            .access_sz = l2cache_line_size,
            .req_time = m_cycle,
            .core_id = -1,
            .warp_id = -1,
            .request_id = n_requests
          };
          n_requests++;
          gpu_mem_request_queue->push(ram_req);
        }

        // Delegated mark dirty in l2
        l2_ins_ln->m_dirty = entry->mark_dirty;

        // Delegated insert in L1
        if(entry->insert_in_l1) {
          // Insert in L1
          core_pointers_v[entry->core_id]->c_l1cache->insert_cache(entry->addr, &line_addr, &victim_line_addr, 0, false);

          //////////////////////////////////////////////////////////////////////////////////////////////////////////////
          // TODO: Task 2.1b: Insert the tag in warp's VTA entry upon L1 eviction.
          // Steps:
          //  - Get tag corresponding to the address. (see if any of the cache class methods can help with this)
          //  - Search for the warp that issued the request in core's (entry->core_id) suspended queue and Insert 
          //    the tag in warp's VTA entry
          if(victim_line_addr) {
            // Get the tag from the address
            Addr repl_ln_tag;
            int dummy_set;
            core_c* core__ = core_pointers_v[entry->core_id];
            core__->c_l1cache->find_tag_and_set(victim_line_addr, &repl_ln_tag, &dummy_set);
            warp_s* warp = core__->c_suspended_warps[entry->warp_id];
            warp->ccws_vta_entry->insert(repl_ln_tag); 

            // Get the warp pointer from suspended queue of core (use core_id from entry->core_id)

            // Insert the tag into the warp's VTA
            CCWSLOG(printf("VTA insertion: %llx\n", repl_ln_tag));

          }
          //////////////////////////////////////////////////////////////////////////////////////////////////////////////
        }

        // Finally insert response in core responses queue
        core_pointers_v[response.core_id]->c_memory_responses.push(response.warp_id);
        
        // erase scoreboard entry
        GPU_scoreboard.erase(entry);
        break;
      }
    }
    total_latency+=resp_time; // Log GPU round trip latency

    if (mem_response_id % 1000 == 0) 
      MA_DEBUG2("RAM resp id:" << response.request_id << " m_cycle=" << m_cycle << " req_time=" 
      << req_time << " resp_time=" << resp_time << " total_latency=" << total_latency);
  }
}

bool macsim::run_a_cycle(){
  if (gpu_retired) return false;
  m_cycle++;

  if (kernel_starting) start_kernel();

  // Run cores
  for (int core_id = 0; core_id < n_of_cores; core_id++) {
    core_c* core = core_pointers_v[core_id];
    if (core->is_retired()) continue;

    core->run_a_cycle();
  }

  // Check whether there is a response in latency tracker queue. If so, 
  // send the reply to the cores
  get_mem_response();

  // Timeout resolution: check MEM request queue at interval=t, if outstanding 
  // time > 10*average latency, report timeout request (print at end)
  if (m_cycle % 100000 == 0) {
    auto entry = GPU_scoreboard.begin();
    while (entry != GPU_scoreboard.end()) {
      uint32_t wait_time = m_cycle - entry->req_time;
      if ((n_responses >= 2000) && (wait_time > (get_avg_latency()*1000))) {
        // if timeout entry found, clear the entry
        n_timeout_req++;
        PRINT_MESSAGE("Timed out entry in core " << entry->core_id << ": m_cycle=" << m_cycle << " entry->req_time="
          << entry->req_time << " wait_time=" << wait_time<< " avg_latency=" << get_avg_latency() << "ns");

        // respond to cores
        core_pointers_v[entry->core_id]->c_memory_responses.push(entry->warp_id);
        GPU_scoreboard.erase(entry);
      } else {
        entry++;
      }
    }
  }

  if (is_every_core_retired()) kernel_ending = true;

  if(kernel_ending){
    end_kernel();
    if (kernel_id >= (int)kernels_v.size()){
      gpu_retired = true;
      cout << "GPU Retired." << endl;
      return false;
    }
  }
  return true;
}

void macsim::start_kernel(){
  cout << "========== starting kernel " << kernel_id << " ==========" << endl;

  // Setup L2 Cache (the size will be the twice of the l1 cache's total size)
  l2cache = new cache_c("dcache", l2cache_size, l2cache_assoc, l2cache_line_size,
                                  sizeof(cache_data_t), l2cache_banks, false, -1, CACHE_DL2, false, 1, 0, this);

  // Setup Cores
  for (int core_id = 0; core_id < n_of_cores; core_id++) {
    core_c* core = new core_c(this, core_id, m_cycle);
    core_pointers_v.push_back(core);
    core->attach_l2_cache(l2cache);
  }

  // Pool and memory allocation
  trace_node_pool = new pool_c<warp_trace_info_node_s>(10, "warp_node_pool");
  warp_pool = new pool_c<warp_s>(10, "warp_pool");

  m_block_queue = new unordered_map<int, list<warp_trace_info_node_s *> *>;

  // Setup blocks and threads
  for (int warp_id = 0; warp_id < kernel_info_v[kernel_id].n_of_warp; warp_id++){
    create_warp_node(kernel_id, get<0>(kernel_info_v[kernel_id].warp_id_v[warp_id]));
  }
  block_scheduling_policy = m_gpu_params->Block_Scheduling_Policy;

  dispatch_warps(-1, block_scheduling_policy);
  kernel_starting = false;
}

void macsim::end_kernel(){
  vector<pair<int, int>> mem_req_v;
  // Retire cores
  for (int core_id = 0; core_id < n_of_cores; core_id++) {
    core_c* core = core_pointers_v[core_id];
    c_cycle_total[core_id] = core->get_cycle();  // Initialialized in start_kernel
    c_insts_total[core_id] += core->get_insts();
    c_stall_cycles[core_id] += core->get_stall_cycles();
    mem_req_v.push_back(make_pair(core->ld_req_cnt, core->st_req_cnt));
    delete core;
  }
  core_pointers_v.clear();
  delete l2cache;

  cout << "========== kernel " << kernel_id << " summary ==========" << endl;
  for (int core_id = 0; core_id < n_of_cores; core_id++){
    printf("Core: %d\n", core_id);
    printf("\tCORE%d_TOT_CYCLES   : %lu\n", core_id, c_cycle_total[core_id]);
    printf("\tCORE%d_STALL_CYCLES : %lu\n", core_id, c_stall_cycles[core_id]);
    printf("\tCORE%d_LD_INSTR     : %u\n", core_id, mem_req_v[core_id].first);
    printf("\tCORE%d_ST_INSTR     : %u\n", core_id, mem_req_v[core_id].second);
    printf("\tCORE%d_TOT_INSTR    : %lu\n", core_id, c_cycle_total[core_id]);
  }

  sim_time_type maxCycleValue = std::numeric_limits<sim_time_type>::min();
  for (const auto& pair : c_cycle_total) {
    if (pair.second > maxCycleValue)
      maxCycleValue = pair.second;
  }

  // Pool deallocation
  delete trace_node_pool;
  delete warp_pool;
  delete m_block_queue;

  m_kernel_block_start_count += kernel_info_v[kernel_id].n_of_block;
  if (kernel_id < (int)kernels_v.size()){
    kernel_id++;
    kernel_starting = true;
    kernel_ending = false;
  }
}

void macsim::create_warp_node(int kernel_id, int warp_id){
  warp_trace_info_node_s *node = trace_node_pool->acquire_entry();

  node->trace_info_ptr = NULL; /**< trace information pointer */
  node->warp_id = warp_id; /**< warp id */
  node->unique_block_id = (warp_id >> 16) + m_kernel_block_start_count; /**< unique block id */

  m_block_list[node->unique_block_id] = true;
  insert_block(node);
  m_num_active_warps++;
}

void macsim::insert_block(warp_trace_info_node_s *node){
  ++m_num_waiting_dispatched_warps;
  int block_id = node->unique_block_id;
  if (m_block_schedule_info.find(block_id) == m_block_schedule_info.end()) {
    block_schedule_info_s *block_schedule_info = new block_schedule_info_s;
    m_block_schedule_info[block_id] = block_schedule_info;
  }
  ++m_block_schedule_info[block_id]->total_thread_num;
  m_block_schedule_info[block_id]->trace_exist = true;

  if (m_block_queue->find(block_id) == m_block_queue->end()) {
    list<warp_trace_info_node_s *> *new_list = new list<warp_trace_info_node_s *>;
    (*m_block_queue)[block_id] = new_list;
  }

  (*m_block_queue)[block_id]->push_back(node);
}

int macsim::dispatch_warps(int core_id, Block_Scheduling_Policy_Types policy){
  int ndispatched_warps=0;
  for(int core_id_ = 0; core_id_ < n_of_cores; core_id_++){
    if (core_id != -1 && core_id_ != core_id) continue; 
    warp_trace_info_node_s* warp_to_run;
    core_c* core = core_pointers_v[core_id_];

    while(core->get_running_warp_num() < core->get_max_running_warp_num()){
      // Schedule Block
      int block_id = schedule_blocks(core_id_, policy); // if return -1, no new block is assigned

      // Pick a warp from block
      warp_to_run = fetch_warp_from_block(block_id);
      if(!warp_to_run)
        break;  // No warps to schedule

      // Initialize warp
      warp_to_run->trace_info_ptr = initialize_warp(warp_to_run->warp_id);
      m_block_schedule_info[block_id]->dispatched_thread_num++;
      
      // TODO: We need to update our timestamp when we dispatch the warp
      warp_to_run->trace_info_ptr->timestampMarkerGTO = core->get_cycle();

      // We need to initialize VTA entry for the warp (associativity for VTA is defined in macsim.h)
      warp_to_run->trace_info_ptr->ccws_vta_entry = new ccws_vta(CCWS_VTA_ASSOC);
      
      // Assign them a base score (defined in macsim.h)
      warp_to_run->trace_info_ptr->ccws_lls_score = CCWS_LLS_BASE_SCORE;
            
      // Dispatch the warp to the core
      core->c_dispatched_warps.push_back(warp_to_run->trace_info_ptr);
      ndispatched_warps++;
    }
  } 

  return ndispatched_warps;
}

warp_s* macsim::initialize_warp(int warp_id){
  warp_s* trace_info = warp_pool->acquire_entry();

  string kernel_path = kernels_v[kernel_id];
  kernel_path = kernel_path.substr(0, kernel_path.find_last_of('.'));
  kernel_path = kernel_path + "_" + to_string(warp_id) + ".raw";
  trace_info->m_trace_file = gzopen(kernel_path.c_str(), "rb");

  if (trace_info->m_trace_file == NULL) {
    int errnum = errno;
    const char* errmsg = strerror(errnum);
    ASSERTM(0, "error opening trace file: %s, errno=%d, errmsg=%s\n", kernel_path.c_str(), errnum, errmsg);
  }
  
  trace_info->m_file_opened = true;
  trace_info->m_trace_ended = false;
  trace_info->warp_id = warp_id;
  trace_info->block_id = warp_id / (1 << 16) + m_kernel_block_start_count; 
  return trace_info;
}

int macsim::schedule_blocks(int core_id, Block_Scheduling_Policy_Types policy){
  switch (policy){
    case Block_Scheduling_Policy_Types::ROUND_ROBIN:
      return schedule_blocks_rr(core_id);
    default:
      ASSERTM(0, "Scheduling Policy not valid!");
      return -1;
  }
}

int macsim::schedule_blocks_rr(int core_id){
  core_c* core = core_pointers_v[core_id];
  int fetching_block_id = core->c_fetching_block_id;

  // Check whether the current block has more warps to execute
  if (fetching_block_id != -1){
    list<warp_trace_info_node_s *> *block_list = (*m_block_queue)[fetching_block_id];
    if (block_list->empty() && !m_block_schedule_info[fetching_block_id]->retired){
      // check every warp that is suspended. if any of them belong to fetching_block_id, return -1
      for (const auto& pair: core->c_suspended_warps){
        if (pair.second->block_id == fetching_block_id) return -1;
      }
      m_block_schedule_info[fetching_block_id]->retired = true;
      core->c_running_block_num--;
    }
  }

  // If the current block that the core is fetching is not retired, return the current block id
  if ((fetching_block_id != -1) &&
      m_block_schedule_info.find(fetching_block_id) != m_block_schedule_info.end() &&
      !(m_block_schedule_info[fetching_block_id]->retired)) {
    return fetching_block_id;
  }

  // If there is not enough space in dispatch queue to enqueue all threads of a block, we return -1
  if (core->c_running_block_num >= max_block_per_core) return -1;
  
  // Find a new block to schedule
  int new_block_id = -1;
  for (auto I = m_block_list.begin(), E = m_block_list.end(); I != E; ++I) {
    int block_id = (*I).first;
    // this block was not fetched yet and has traces to schedule
    if (!(m_block_schedule_info[block_id]->start_to_fetch) &&
        m_block_schedule_info[block_id]->trace_exist) {
      new_block_id = block_id;
      break;
    }
  }
  if (new_block_id == -1) return -1;
  else {
    m_block_schedule_info[new_block_id]->start_to_fetch = true;
    m_block_schedule_info[new_block_id]->dispatched_core_id = core_id;
    core->c_running_block_num++;
    core->c_fetching_block_id = new_block_id;
    return new_block_id;
  }
}

warp_trace_info_node_s* macsim::fetch_warp_from_block(int block_id) {
  if (block_id == -1) return NULL;
  list<warp_trace_info_node_s *> *block_list = (*m_block_queue)[block_id];
  assert(m_num_waiting_dispatched_warps > 0);
  --m_num_waiting_dispatched_warps;

  warp_trace_info_node_s *front = block_list->front();
  block_list->pop_front();
  return front;
}

bool macsim::is_every_core_retired() {
  for (int core_id = 0; core_id < n_of_cores; core_id++)
    if (!core_pointers_v[core_id]->is_retired()) return false;
  return true;
}

void macsim::print_stats() {
  uint64_t n_total_stall_cycles = 0;
  for(auto x: c_stall_cycles) {
    n_total_stall_cycles += x.second;
  }

  uint64_t n_total_instrs_retired=0;
  for(auto x: c_insts_total) {
    n_total_instrs_retired += x.second;
  }

  printf("\n============= MacSim Stats =============\n");
  printf("Macsim:\n");
  printf("\tNUM_CYCLES              : %lu\n", m_cycle);
  printf("\tNUM_INSTRS_RETIRED      : %lu\n", n_total_instrs_retired);
  printf("\tNUM_STALL_CYCLES        : %lu\n", n_total_stall_cycles);    
  printf("\tNUM_MEM_REQUESTS        : %lu\n", n_requests);
  printf("\tNUM_MEM_RESPONSES       : %lu\n", n_responses);
  printf("\tAVG_RESPONSE_LATENCY    : %lu\n", total_latency/n_responses);
  printf("\tNUM_TTIMEDOUT_REQUESTS  : %lu\n", n_timeout_req);
  
  float ipc = (float)n_total_instrs_retired/(float)m_cycle;
  printf("\tINSTR_PER_CYCLE         : %lf\n", ipc);
  
  printf("Cache:\n");
  if (m_gpu_params->Enable_GPU_Cache) {
    printf("\tCACHE_NUM_ACCESSES    : %lu\n", n_cache_req);
    printf("\tCACHE_NUM_HITS        : %lu\n", n_l1_hits);
    printf("\tCACHE_HIT_RATE_PERC   : %.2f\n", ((float)n_l1_hits*100.0) / (float)n_cache_req); // hit rate = n_hits * 100 / total cache accesses
    
    float mpki = (float)(n_cache_req - n_l1_hits) * 1000.0 / (float)n_total_instrs_retired;
    printf("\tMISSES_PER_1000_INSTR : %.2f\n", mpki);
  }
  else { 
      PRINT_MESSAGE("GPU cache disabled");
  }
  printf("\n========================================\n");
}

void macsim::end_sim(){
  // End of simulation: collect stats from response lantency log to verify
  PRINT_MESSAGE("");
  PRINT_MESSAGE("[END OF GPU SIMULATION]");
}
